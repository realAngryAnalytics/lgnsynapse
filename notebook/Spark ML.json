{
	"name": "Spark ML",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c5cfd151-2cd1-4e9c-8905-f8fa2cdd66f9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"## Making Product Recommendations\n",
					"\n",
					"In this notebook, you will use sales data to create product recommendations for customers. \n",
					"\n",
					"When creating recommendation models there are generally two approaches that vary only on the data you use to compute the \"strength\" of a recommendation:\n",
					"- **Explicit ratings**: In this case, each user and product has a star rating attached. Think you might review restaurants in your favorite app.\n",
					"- **Implicit ratings**: In this case each user and product has a rating that is derived from some behavioral metric. Typically that metric is a count, like number purchases of that product or the number of product page views. The actual rating is \"implicit\" in the sense that it is computed algorithmically instead of directly using the value provided.\n",
					"\n",
					"In this notebook, you train a model that makes product recommendation based purchase history. For each user and product that appears in the history, you will sum the quantity of items purchased across each transaction. This sum will create an **explicit** rating for the user to product mapping, effectively your model is saying the more of a product a user buys across all transactions, the more relevant it is to that user.\n",
					"\n",
					"The model then goes one step further, and enables you to calculate the recommendations for a user, who may not have bought the product before, but because her purchases are similar to another's, she might like the strongest recommendations inferred from other users like her. Think of this as the algorthim filling in the blanks for the user and a given product, it predicts what that rating should be between them.\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Using Spark ML's ALS Algorithm\n",
					"\n",
					"Run the following cell to import the ALS class.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.ml.recommendation import ALS\n",
					"from pyspark.sql import Row\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"Run the following cell to load the SaleSmall table from the SQL Pool. Make sure the SQL pool name (#SQL_POOL_NAME#) matches the name of your SQL Pool.\n",
					"\n",
					"Remember, that in order to read from table like this, we need to use Scala to create a DataFrame around it.\n",
					"\n",
					"Once you have the DataFrame, you can create a named view from it. Since named views are registered in the shared metastore, you can access view by name from both Scala and Python cells.\n",
					"\n",
					"In this cell we use the Spark magic to run this cells content's using Scala, create the DataFrame and then register it as a view that we will use from later Python cells.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%spark\n",
					"val df = spark.read.sqlanalytics(\"#SQL_POOL_NAME#.wwi.SaleSmall\") \n",
					"df.createOrReplaceTempView(\"sales\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"Next, you need to get just the data you need to train the recommender.\n",
					"\n",
					"A recommender really only operates on three fields: the customerid/userid, the itemid/productid and the rating.\n",
					"\n",
					"In the following cell, we select just those columns, summing the quantity field to get a toal purchase count for any given product by that user in the history. This field, which we alias as numpurchases, is our rating.\n",
					"\n",
					"Then we call `cache()` on the dataframe so that resultant dataset is cached in Spark memory (or disk) and does not have to be recomputed everytime we make a subequent query for it. Machine learning algorithms like ALS make several passes thru data, so caching the DataFrame provides a significant performance boost.\n",
					"\n",
					"Run the following cell to prepare and cached the implicit ratings (IR) table.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# To be able to complete this lab in under an hour, let's just work with 1,500k rows\n",
					"sample = spark.sql(\"SELECT productid, customerid, quantity FROM sales\").limit(15000000)\n",
					"sample.createOrReplaceTempView(\"salessample\")\n",
					"\n",
					"ir = spark.sql(\"SELECT productid, customerid, SUM(quantity) as numpurchases FROM salessample GROUP BY productid, customerid LIMIT 500000\")\n",
					"ir.cache()\n",
					"display(ir)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"In training a model, we withold a subset of the data to use when evaluating the model. This is the test set. The train set is what we show to the algorithm for it to learn from.\n",
					"\n",
					"The basic idea is if you show the model all of the data you have, it may effectively memorize all the answers- meaning it will perform very well if it has seen the question before, but probably perform poorly against new questions. This problem that is avoided is called overfitting the model to the data.\n",
					"\n",
					"In the first line, we split our data into those train and test subsets.\n",
					"\n",
					"In the sceond line, we instantiate the ALS algorithm, telling it which columns in our data are the user, item and rating. THe `maxIter` controls how many passes the training takes over the data, and the `regParam` controls how big of an adjustment the model makes during each pass. These are set to the common defaults so you can ignore those values for the purposes of this lab. \n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"train, test = ir.randomSplit([0.7,0.3])\n",
					"als = ALS(maxIter=5,regParam=0.01,userCol=\"customerid\",itemCol=\"productid\",ratingCol=\"numpurchases\")\n",
					"model = als.fit(train)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"With a trained model in hand, we can now use it to make some recommendations. \n",
					"\n",
					"In reccommender systems it is very common to pre-compute the recommendations in batch and then simply query their reccommendations for a single customer later.\n",
					"\n",
					"Run the following cell to batch compute the top 5 product recommendations for each customer, and see a sampling of the result.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"product_recommendations = model.recommendForAllUsers(5).selectExpr(\"customerid\",\"explode(recommendations) as rec\")\n",
					"product_recommendations = product_recommendations.selectExpr(\"customerId\", \"rec.productid\", \"rec.rating\")\n",
					"product_recommendations.createOrReplaceTempView(\"recommendations\")\n",
					"display(product_recommendations)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"Now that you have the batch recommendations, save the results to a table in a SQL Pool by running the following cell. With this, downstream applications can look up recommendations by issuing a traditional T-SQL query.\n",
					"\n",
					"Don't forget to check the name of your SQL Pool used on Line 3.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\n",
					"val recommendations = spark.sql(\"SELECT * from recommendations\")\n",
					"recommendations.write.sqlanalytics(\"#SQL_POOL_NAME#.wwi.Recommendations\", Constants.INTERNAL) "
				],
				"execution_count": null
			}
		]
	}
}