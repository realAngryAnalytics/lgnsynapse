{
	"name": "04 Using Delta Lake in Azure Synapse",
	"properties": {
		"folder": {
			"name": "PySpark"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "66c08f9b-2a56-4af9-9444-71a5fab5d97a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Using Linux Foundation Delta Lake in Azure Synapse Analytics Spark\n",
					"Azure Synapse is compatible with Linux Foundation Delta Lake. Delta Lake is an open-source storage layer that brings ACID (atomicity, consistency, isolation, and durability) transactions to Apache Spark and big data workloads.\n",
					"\n",
					"This notebook provides examples of how to update, merge and delete delta lake tables in Synapse.\n",
					"\n",
					"## Pre-requisites\n",
					"In this notebook you will save your tables in Delta Lake format to your workspace's primary storage account. You are required to be a **Blob Storage Contributor** in the ADLS Gen2 account (or folder) you will access.\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Load sample data\n",
					"\n",
					"First you will load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) data from last 6 months via Azure Open datasets.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.opendatasets import PublicHolidays\n",
					"\n",
					"from datetime import datetime\n",
					"from dateutil import parser\n",
					"from dateutil.relativedelta import relativedelta\n",
					"\n",
					"\n",
					"end_date = datetime.today()\n",
					"start_date = datetime.today() - relativedelta(months=6)\n",
					"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
					"hol_df = hol.to_spark_dataframe()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"display(hol_df)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Write data to the Delta Lake table\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Set the strorage path info\n",
					"# Primary storage info\n",
					"account_name = '' # fill in your primary storage account name\n",
					"container_name = '' # fill in your container name\n",
					"relative_path = '' # fill in your relative folder path\n",
					"\n",
					"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
					"print('Primary storage account path: ' + adls_path)\n",
					"\n",
					"# Delta Lake relative path\n",
					"delta_relative_path = adls_path + 'delta/holiday/'\n",
					"print('Delta Lake path: ' + delta_relative_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Filter out indian holidays\n",
					"hol_df_IN = hol_df[(hol_df.countryRegionCode == \"IN\")]\n",
					"hol_df_IN.show(5, truncate = False)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					"#Let's write the data in the Delta Lake table. \n",
					"hol_df_IN.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"holidayName\").save(delta_relative_path)"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					"delta_data = spark.read.format(\"delta\").load(delta_relative_path)\n",
					"delta_data.show()"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Overwrite the entire Delta Lake table\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Let's overwrite the entire delta file with 1 record\n",
					"\n",
					"hol_df_JP= hol_df[(hol_df.countryRegionCode == \"JP\")]\n",
					"hol_df_JP.write.format(\"delta\").mode(\"overwrite\").save(delta_relative_path)"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"delta_data = spark.read.format(\"delta\").load(delta_relative_path)\n",
					"delta_data.show()"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Merge new data based on given merge condition "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Upsert (merge) the United States' holiday data with Japan's\n",
					" \n",
					"from delta.tables import *\n",
					"\n",
					"deltaTable = DeltaTable.forPath(spark,delta_relative_path)\n",
					"\n",
					"hol_df_US= hol_df[(hol_df.countryRegionCode == \"US\")]\n",
					"\n",
					"\n",
					"deltaTable.alias(\"hol_df_JP\").merge(\n",
					"     source = hol_df_US.alias(\"hol_df_US\"),\n",
					"     condition = \"hol_df_JP.countryRegionCode = hol_df_US.countryRegionCode\"\n",
					"    ).whenMatchedUpdate(set = \n",
					"    {}).whenNotMatchedInsert( values = \n",
					"    {\n",
					"        \"countryOrRegion\" : \"hol_df_US.countryOrRegion\",\n",
					"        \"holidayName\" : \"hol_df_US.holidayName\",\n",
					"        \"normalizeHolidayName\" : \"hol_df_US.normalizeHolidayName\",\n",
					"        \"isPaidTimeOff\":\"hol_df_US.isPaidTimeOff\",\n",
					"        \"countryRegionCode\":\"hol_df_US.countryRegionCode\",\n",
					"        \"date\":\"hol_df_US.date\"\n",
					"    }\n",
					"    ).execute()\n",
					"\n",
					"\n",
					"deltaTable.toDF().show()"
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Update table on the rows that match the given condition\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Update column the 'null' value in 'isPaidTimeOff' with 'false'\n",
					"\n",
					"from pyspark.sql.functions import *\n",
					"deltaTable.update(\n",
					"    condition = (col(\"isPaidTimeOff\").isNull()),\n",
					"    set = {\"isPaidTimeOff\": \"false\"})\n",
					"\n",
					"deltaTable.toDF().show()"
				],
				"execution_count": 71
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Delete data from the table that match the given condition\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"print(\"Row count before delete: \")\n",
					"print(deltaTable.toDF().count())\n",
					"\n",
					"\n",
					"# Delte data with date later than 2020-01-01\n",
					"deltaTable.delete (\"date > '2020-01-01'\")\n",
					"\n",
					"\n",
					"print(\"Row count after delete:  \")\n",
					"print(deltaTable.toDF().count())\n",
					"deltaTable.toDF().show()"
				],
				"execution_count": 72
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Get the operation history of the delta table\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"fullHistoryDF = deltaTable.history()\n",
					"lastOperationDF = deltaTable.history(1)\n",
					"\n",
					"print('Full history DF: ')\n",
					"fullHistoryDF.show(truncate = False)\n",
					"\n",
					"print('lastOperationDF: ')\n",
					"lastOperationDF.show(truncate = False)"
				],
				"execution_count": 73
			}
		]
	}
}